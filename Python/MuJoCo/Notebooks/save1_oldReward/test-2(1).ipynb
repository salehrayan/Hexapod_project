{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!conda install -y gdown\n!gdown 1i6gsJch1_Odnz-ddqGQzpakD5mowwx-6","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!apt install -y python-opengl ffmpeg > /dev/null 2>&1\n\n# !apt install -y xvfb\n%pip install pyvirtualdisplay\nfrom pyvirtualdisplay import Display\nDisplay(visible=False, size=(1400, 900)).start()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install -U \"jax[cuda12]\"\n!pip install mujoco\n!pip install mujoco_mjx\n!pip install brax","metadata":{"id":"hMKMX5Gpll8p","outputId":"cd49772d-9556-42fa-cd7f-b18c31a3cb3d","_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Check if MuJoCo installation was successful\nimport os\n\n\n# Configure MuJoCo to use the EGL rendering backend (requires GPU)\nprint('Setting environment variable to use GPU rendering:')\n%env MUJOCO_GL=glfw\n\ntry:\n  print('Checking that the installation succeeded:')\n  import mujoco\n  mujoco.MjModel.from_xml_string('<mujoco/>')\nexcept Exception as e:\n  raise e from RuntimeError(\n      'Something went wrong during installation. Check the shell output above '\n      'for more information.\\n'\n      'If using a hosted Colab runtime, make sure you enable GPU acceleration '\n      'by going to the Runtime menu and selecting \"Choose runtime type\".')\n\nprint('Installation successful.')\n\n# Tell XLA to use Triton GEMM, this improves steps/sec by ~30% on some GPUs\nos.environ['XLA_FLAGS'] = (\n    '--xla_gpu_enable_triton_softmax_fusion=true '\n    '--xla_gpu_triton_gemm_any=True '\n    '--xla_gpu_enable_async_collectives=true '\n    '--xla_gpu_enable_latency_hiding_scheduler=true '\n    '--xla_gpu_enable_highest_priority_async_stream=true '\n)\n","metadata":{"cellView":"form","id":"0x7XTKjPmtR8","outputId":"7d085686-c173-4d40-aed3-d2fc0ad95379","execution":{"iopub.status.busy":"2024-06-24T23:52:15.291840Z","iopub.execute_input":"2024-06-24T23:52:15.292781Z","iopub.status.idle":"2024-06-24T23:52:15.499938Z","shell.execute_reply.started":"2024-06-24T23:52:15.292744Z","shell.execute_reply":"2024-06-24T23:52:15.498897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!sudo apt install unrar\n!pip install patool\nimport patoolib\npatoolib.extract_archive(\"/kaggle/working/Spider.rar\",outdir='/kaggle/working/')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Import packages for plotting and creating graphics\nimport time\nimport itertools\nimport numpy as np\nimport random\nfrom typing import Callable, NamedTuple, Optional, Union, List\n\n# Graphics and plotting.\nprint('Installing mediapy:')\n# !command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)\n!pip install -q mediapy\nimport mediapy as media\nimport matplotlib.pyplot as plt\n\n# More legible printing from numpy.\nnp.set_printoptions(precision=3, suppress=True, linewidth=100)","metadata":{"cellView":"form","id":"qZQGSUePmux_","outputId":"5174b9c9-07bd-4f16-c131-a7274f697caa","execution":{"iopub.status.busy":"2024-06-24T23:52:17.849508Z","iopub.execute_input":"2024-06-24T23:52:17.849854Z","iopub.status.idle":"2024-06-24T23:52:31.135389Z","shell.execute_reply.started":"2024-06-24T23:52:17.849827Z","shell.execute_reply":"2024-06-24T23:52:31.133617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Import MuJoCo, MJX, and Brax\nfrom datetime import datetime\nfrom etils import epath\nimport functools\nfrom IPython.display import HTML\nfrom typing import Any, Dict, Sequence, Tuple, Union\nimport os\nfrom ml_collections import config_dict\nimport math\n\n\nimport jax\nfrom jax import numpy as jp\nimport numpy as np\nfrom flax.training import orbax_utils\nfrom flax import struct\nfrom matplotlib import pyplot as plt\nimport mediapy as media\nfrom orbax import checkpoint as ocp\n\nimport mujoco\nfrom mujoco import mjx\n\nfrom brax import base\nfrom brax import envs\n# from brax import math\nfrom brax.base import Base, Motion, Transform\nfrom brax.envs.base import Env, PipelineEnv, State\nfrom brax.mjx.base import State as MjxState\nfrom brax.training.agents.ppo import train as ppo\nfrom brax.training.agents.es import train as es\nfrom brax.training.agents.ppo import networks as ppo_networks\nfrom brax.training.agents.es import networks as es_networks\nfrom brax.io import html, mjcf, model\n","metadata":{"cellView":"form","id":"V8Sbq87gm2Mn","execution":{"iopub.status.busy":"2024-06-24T23:52:31.144052Z","iopub.execute_input":"2024-06-24T23:52:31.148012Z","iopub.status.idle":"2024-06-24T23:52:33.527541Z","shell.execute_reply.started":"2024-06-24T23:52:31.147951Z","shell.execute_reply":"2024-06-24T23:52:33.526613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xml_path = r'/kaggle/working/Spider_Assembly_fineMesh_frictionDamp/urdf/final_noFrictionLoss_noCoxaCon_explicitConPair_ellipsoidTibias_kaggle.xml'\n\nclass HexapodV0_3(PipelineEnv):\n    def __init__(self,\n                 xml_path,\n                 terminateWhenTilt=True,\n                 terminateWhenTiltGreaterThan=30 * math.pi / 180,\n                 baseTiltSigma=0.015,\n                 baseTiltCoef=2,\n\n                 terminateWhenFumersColide=False,\n                 femurCollisionSigma=0.06,\n                 femurCollisionCoef=0.5,\n\n                 correctDirectionSigma=0.5,\n                 correctDirectionWeight=1,\n                 deviationAngleSigma=0.03,\n                 deviationAngleWeight=1,\n\n                 baseHeightSigma=0.027,\n                 baseHeightCoef=1,\n                 terminateWhenLow=True,\n                 baseHeightLowerLimit=0.15,\n                 baseOscillationSigma=0.02075,\n                 baseOscillationCoef=1.5,\n\n                 rewardForTibiaTip=True,\n                 tibiaRewardSigma=0.05,\n                 tibiaRewardCoef=1,\n\n                 powerCoef=0.001,\n                 continuityCoef=0.05,\n\n                 includeBaseAngularVels=True,\n                 includeTibiaTipSensors=False,\n                 nStacks=3,\n                 physics_steps_per_control_step=10,\n\n                 resetPosLowHigh=[jp.array([-0.2, -0.2, 0.23]), jp.array([0.2, 0.2, 0.4])],\n                 resetOriLowHigh=[jp.array([-math.pi/12, -math.pi/12, -math.pi]), jp.array([math.pi/12, math.pi/12, -math.pi])],\n                 resetJointsPosLowHigh = [jp.array([-math.pi/12]*18), jp.array([math.pi/12]*18)],\n                 resetJointsVelsLowHigh=[jp.array([-0.3]*24), jp.array([-0.3]*24)],\n                 **kwargs\n                 ):\n\n        self.mj_model = mujoco.MjModel.from_xml_path(xml_path)\n        self.mj_model.opt.solver = mujoco.mjtSolver.mjSOL_NEWTON\n        self.mj_model.opt.iterations = 6\n        self.mj_model.opt.ls_iterations = 6\n\n        sys = mjcf.load_model(self.mj_model)\n\n        kwargs['n_frames'] = kwargs.get(\n            'n_frames', physics_steps_per_control_step)\n        kwargs['backend'] = 'mjx'\n        super().__init__(sys, **kwargs)\n\n        self._terminateWhenTilt = terminateWhenTilt\n        self._terminateWhenTiltGreaterThan = terminateWhenTiltGreaterThan\n        self._baseTiltSigma= baseTiltSigma\n        self._baseTiltCoef= baseTiltCoef\n        self._terminateWhenFumersColide = terminateWhenFumersColide\n        self._femurCollisionSigma = femurCollisionSigma\n        self._femurCollisionCoef = femurCollisionCoef\n        self._correctDirectionSigma = correctDirectionSigma\n        self._correctDirectionWeight = correctDirectionWeight\n        self._deviationAngleSigma = deviationAngleSigma\n        self._deviationAngleWeight = deviationAngleWeight\n        self._baseHeightSigma = baseHeightSigma\n        self._baseHeightCoef = baseHeightCoef\n        self._terminateWhenLow = terminateWhenLow\n        self._baseHeightLowerLimit = baseHeightLowerLimit\n        self._baseOscillationSigma = baseOscillationSigma\n        self._baseOscillationCoef = baseOscillationCoef\n        self._rewardForTibiaTip = rewardForTibiaTip\n        self._tibiaRewardSigma = tibiaRewardSigma\n        self._tibiaRewardCoef = tibiaRewardCoef\n        self._powerCoef = powerCoef\n        self._continuityCoef = continuityCoef\n        self._includeBaseAngularVels = includeBaseAngularVels\n        self._includeTibiaTipSensors = includeTibiaTipSensors\n        self._resetPosLowHigh = resetPosLowHigh\n        self._resetOriLowHigh = resetOriLowHigh\n        self._resetJointsPosLowHigh = resetJointsPosLowHigh\n        self._resetJointsVelsLowHigh = resetJointsVelsLowHigh\n        self._includeBaseAngularVels = includeBaseAngularVels\n        self._includeTibiaTipSensors = includeTibiaTipSensors\n        self._nStacks = nStacks\n        self._physics_steps_per_control_step = physics_steps_per_control_step\n\n    def reset(self, rng: jp.ndarray) -> State:\n        \"\"\"Resets the environment to an initial state.\"\"\"\n        rng, rng1, rng2, rng3, rng4, numTransitionsRng, desiredVelRng, desiredAngleRng, transitionStepsRng= jax.random.split(rng, 9)\n\n        base_pos = jax.random.uniform(key=rng1, shape=(3,), minval=self._resetPosLowHigh[0], maxval=self._resetPosLowHigh[1])\n        # print(base_pos)\n        base_orientation_euler = jax.random.uniform(key=rng2, shape=(3,), minval=self._resetOriLowHigh[0],\n                                                    maxval=self._resetOriLowHigh[1])\n        base_orientation = self._euler_to_quaternion(base_orientation_euler)\n\n        joints_pos = jax.random.uniform(key=rng3, shape=(18,), minval=self._resetJointsPosLowHigh[0],\n                                                    maxval=self._resetJointsPosLowHigh[1])\n        qpos = jp.concatenate((base_pos, base_orientation, joints_pos), axis=0)\n\n        qvel = jax.random.uniform(key=rng4, shape=(24,), minval=self._resetJointsVelsLowHigh[0],\n                                                    maxval=self._resetJointsVelsLowHigh[1])\n\n        data = self.pipeline_init(qpos, qvel)\n\n        obs_history = jp.zeros(self._nStacks * (self._includeBaseAngularVels * 3 + self._includeTibiaTipSensors * 6 + 18  + 2))\n        reward, done, zero = jp.zeros(3)\n        \n#         num_transitions = jax.random.randint(key=numTransitionsRng, shape=(1,), minval=1, maxval=11)[0].astype(int)\n        num_transitions = random.randint(1, 10)\n        desired_vels = jax.random.uniform(key=desiredVelRng, shape=(num_transitions + 1,), minval=0.2, maxval=1.5)\n        desired_angles = jax.random.uniform(key=desiredVelRng, shape=(num_transitions + 1,), minval=-math.pi, maxval=math.pi)\n        transition_steps = jax.random.randint(key=transitionStepsRng, shape=(num_transitions,), minval=50, maxval=951)\n\n        obs= self._get_obs(data, jp.zeros(self.sys.nu), desired_vels[0], desired_angles[0], obs_history)\n        state_info = {\n            'last_action': jp.zeros(self.sys.nu),\n            'num_transitions': num_transitions,\n            'desired_vels': desired_vels,\n            'desired_angles': desired_angles,\n            'transition_steps': transition_steps,\n            'current_idx' : 0,\n            'step': 0,\n        }\n        metrics = {\n            'correct_direction_reward': zero,\n            'base_tilt':zero,\n            'base_height': zero,\n            'movement_angle': zero,\n            'continuity': zero,\n            'power': zero,\n            'tibia_tip_contact': zero,\n            'femur_collision': zero,\n            'x_position': zero,\n            'y_position': zero,\n            'distance_from_origin': zero,\n            'x_velocity': zero,\n            'y_velocity': zero,\n            'base_tilt_reward': zero,\n            'tibia_reward': zero,\n            'total_reward' : zero\n        }\n        return State(data, obs, reward, done, metrics, state_info)\n\n    def step(self, state: State, action: jp.ndarray) -> State:\n        \"\"\"Runs one timestep of the environment's dynamics.\"\"\"\n        prev_pipeline_state = state.pipeline_state\n        pipeline_state = self.pipeline_step(prev_pipeline_state, action)\n        last_action = state.info['last_action']\n        desired_vel = state.info['desired_vels'][state.info['current_idx']]\n        desired_angle = state.info['desired_angles'][state.info['current_idx']]\n        desired_velx = desired_vel * jp.cos(desired_angle)\n        desired_vely = desired_vel * jp.sin(desired_angle)\n\n\n        # prev_base_pos = prev_pipeline_state.x.pos[0,:]\n        prev_base_pos = prev_pipeline_state.subtree_com[0]\n        # base_pos = pipeline_state.x.pos[0,:]\n        base_pos = pipeline_state.subtree_com[0]\n        displacement = base_pos - prev_base_pos\n        velocity = displacement / self.dt\n        \n        base_ori = pipeline_state.x.rot[0,:]\n        base_tilt = (jp.linalg.norm(base_ori[0:2]))\n        base_ang_vel = jp.linalg.norm(pipeline_state.xd.ang[0,0:2])\n        movement_angle = jp.atan2(displacement[0], displacement[1])\n        \n        diversion_vector = jp.array([desired_velx - velocity[0], desired_vely - velocity[1]])\n\n\n        correctDirectionReward = self._correctDirectionWeight * jp.exp(-(jp.linalg.norm(diversion_vector))**2/self._correctDirectionSigma**2)\n\n        deviationReward = (self._deviationAngleWeight * jp.exp(-(desired_angle-movement_angle)**2/self._deviationAngleSigma**2))\n\n        femurDistanceReward = self._get_femur_reward(pipeline_state)\n        tibiaReward = self._get_tibia_reward(pipeline_state) * self._rewardForTibiaTip\n\n        # baseHeightReward = (self._baseHeightCoef * jp.exp(-(0.23 - base_pos[2])**2/self._baseHeightSigma**2) *\n                            # (base_pos[2] > self._baseHeightLowerLimit) )\n        baseHeightReward = 0\n        baseOscillationReward = self._baseOscillationCoef * jp.exp(-base_ang_vel**2/\n                                                                   self._baseOscillationSigma**2)\n        baseTiltReward = (self._baseTiltCoef * jp.exp(-base_tilt**2/self._baseTiltSigma**2))\n        termination = jp.array(((base_tilt > self._terminateWhenTiltGreaterThan) * self._terminateWhenTilt |\n                       (base_pos[2] < self._baseHeightLowerLimit) * self._terminateWhenLow), dtype=jp.bool)\n\n        # prev_action = state.metrics['prev_action']\n        continuity_reward = self._continuityCoef * ((action - last_action)**2).sum()\n        state.info['last_action'] = action\n\n        reward = (correctDirectionReward + deviationReward - continuity_reward - femurDistanceReward + baseHeightReward +\n                  baseOscillationReward - 1 * termination + baseTiltReward + tibiaReward)\n        done = 1.0 - ~termination\n        # print('1')\n        state.info['step'] += 1\n        condition = jp.any(state.info['step'] == state.info['transition_steps'])\n        new_current_idx = jax.lax.select(condition, state.info['current_idx'] + 1, state.info['current_idx'])\n\n        state.info['current_idx'] = new_current_idx\n    \n        obs = self._get_obs(pipeline_state,  action, desired_vel, desired_angle, obs_history=state.obs)\n\n        state.metrics.update(correct_direction_reward=correctDirectionReward,\n                             base_tilt=base_tilt,\n                             base_height=base_pos[2],\n                             movement_angle=movement_angle,\n                             continuity=continuity_reward,\n                             x_position=base_pos[0],\n                             y_position=base_pos[1],\n                             distance_from_origin=jp.linalg.norm(base_pos[0:2]),\n                             x_velocity=velocity[0],\n                             y_velocity=velocity[1],\n                             base_tilt_reward=baseTiltReward,\n                             tibia_reward=tibiaReward,\n                             total_reward=reward\n                             )\n        # print(state.metrics)\n        return state.replace(\n            pipeline_state=pipeline_state, obs=obs, reward=reward, done=done\n        )\n\n\n    def _get_obs(\n            self, data: base.State, action: jp.ndarray, desired_vel, desired_angle, obs_history: jax.Array\n    ) -> jp.ndarray:\n        \"\"\"Observes\"\"\"\n        # mjx.forward(self.sys, data)\n        # historic_action = jp.zeros(self.sys.nu*2)\n        current_obs = jp.concatenate((jp.array([desired_vel]), jp.array([desired_angle]), data.qpos[7:]), axis=0)\n        if self._includeBaseAngularVels:\n            base_ang_vel = data.xd.ang[0,:]\n            # print(data.xd.ang)\n            current_obs = jp.append(base_ang_vel, current_obs)\n\n        if self._includeTibiaTipSensors:\n            mj_data = mjx.get_data(self.mj_model, data)\n            for i in range(6):\n                contact = jp.array(mj_data.sensordata[i], dtype=jp.bool)\n                current_obs = jp.append(current_obs, contact)\n\n        obs = jp.roll(obs_history, current_obs.size).at[:current_obs.size].set(current_obs)\n        return obs\n\n    def _get_femur_reward(self, pipeline_state):\n        femur_dists = pipeline_state.contact.dist[6:]\n#         print(pipeline_state.contact.dist)\n#         print(femur_dists)\n        femur_reward = self._femurCollisionCoef * jp.exp(-femur_dists.min()**2/self._femurCollisionSigma**2)\n        return femur_reward\n\n    def _get_tibia_reward(self, pipeline_state: State) -> jp.ndarray:\n        contact_dists = pipeline_state.contact.dist[0:6]\n        contact_booleans = (jp.abs(contact_dists) < 0.03) * jp.ones(6)\n        tibia_tip_dists = 0\n        for i in range(2,8):\n            tibia_tip_dists += contact_booleans[i-2] * pipeline_state.site_xpos[i, 2]\n\n        tibia_reward = self._tibiaRewardCoef * jp.exp(-tibia_tip_dists**2/self._tibiaRewardSigma**2)\n        return tibia_reward\n\n\n    def _euler_to_quaternion(self, euler):\n        \"\"\"Converts Euler angles to quaternion.\"\"\"\n        roll, pitch, yaw = euler\n        qx = jp.sin(roll/2) * jp.cos(pitch/2) * jp.cos(yaw/2) - jp.cos(roll/2) * jp.sin(pitch/2) * jp.sin(yaw/2)\n        qy = jp.cos(roll/2) * jp.sin(pitch/2) * jp.cos(yaw/2) + jp.sin(roll/2) * jp.cos(pitch/2) * jp.sin(yaw/2)\n        qz = jp.cos(roll/2) * jp.cos(pitch/2) * jp.sin(yaw/2) - jp.sin(roll/2) * jp.sin(pitch/2) * jp.cos(yaw/2)\n        qw = jp.cos(roll/2) * jp.cos(pitch/2) * jp.cos(yaw/2) + jp.sin(roll/2) * jp.sin(pitch/2) * jp.sin(yaw/2)\n        return jp.array([qw, qx, qy, qz])\n\n    def render(\n            self, trajectory: List[base.State], camera = None,\n            width: int = 640, height: int = 480,\n    ) -> Sequence[np.ndarray]:\n        camera = camera or 'track'\n        return super().render(trajectory, camera=camera, width=width, height=height)\n\n\n\nenv = HexapodV0_3(xml_path=xml_path)\n","metadata":{"id":"ZkLySe59xXyQ","outputId":"4707f837-d7af-43b0-8c02-8cbac5e67915","execution":{"iopub.status.busy":"2024-06-24T23:52:33.528874Z","iopub.execute_input":"2024-06-24T23:52:33.529350Z","iopub.status.idle":"2024-06-24T23:52:35.344335Z","shell.execute_reply.started":"2024-06-24T23:52:33.529320Z","shell.execute_reply":"2024-06-24T23:52:35.343310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# define the jit reset/step functions\njit_reset = jax.jit(env.reset)\njit_step = jax.jit(env.step)","metadata":{"id":"ZU0WXzJsyCtl","execution":{"iopub.status.busy":"2024-06-24T23:52:35.346786Z","iopub.execute_input":"2024-06-24T23:52:35.347581Z","iopub.status.idle":"2024-06-24T23:52:35.352864Z","shell.execute_reply.started":"2024-06-24T23:52:35.347540Z","shell.execute_reply":"2024-06-24T23:52:35.351834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_networks_factory = functools.partial(\n    ppo_networks.make_ppo_networks,\n        policy_hidden_layer_sizes=(128, 128, 128))\ntrain_fn = functools.partial(\n      ppo.train, num_timesteps=250_000_000, num_evals=20,\n      reward_scaling=1, episode_length=1000, normalize_observations=True,\n      action_repeat=1, unroll_length=20, num_minibatches=64,\n      num_updates_per_batch=4, discounting=0.97, learning_rate=3.0e-4,\n      entropy_cost=1e-2, num_envs=int(8192), batch_size=256,\n      network_factory=make_networks_factory,\n      seed=0)\n\ndef progress(num_steps, metrics):\n  print(metrics['eval/episode_reward'])\n  times.append(datetime.now())\n  plt.figure()\n  x_data.append(num_steps)\n  y_data.append(metrics['eval/episode_reward'])\n  ydataerr.append(metrics['eval/episode_reward_std'])\n\n  plt.xlim([0, train_fn.keywords['num_timesteps'] * 1.25])\n#   plt.ylim([min_y, max_y])\n\n  plt.xlabel('# environment steps')\n  plt.ylabel('reward per episode')\n  plt.title(f'y={y_data[-1]:.3f}')\n\n  plt.errorbar(\n      x_data, y_data, yerr=ydataerr)\n  plt.show()\n\nx_data = []\ny_data = []\nydataerr = []\ntimes = [datetime.now()]\n\nenv = HexapodV0_3(xml_path=xml_path)\neval_env = HexapodV0_3(xml_path=xml_path)\nmake_inference_fn, params, metrics = train_fn(environment=env,\n                                       progress_fn=progress,\n                                       eval_env=eval_env)\n\nprint(f'time to jit: {times[1] - times[0]}')\nprint(f'time to train: {times[-1] - times[1]}')","metadata":{"id":"5zozZrgX3Tr6","outputId":"37338359-d77a-4772-be38-750d5ca513d6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_path = '/kaggle/working/mjx_brax_policy_19'\nmodel.save_params(model_path, params)","metadata":{"id":"QL4iPEJ-Ujt6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = model.load_params(model_path)\n\ninference_fn = make_inference_fn(params)\njit_inference_fn = jax.jit(inference_fn)","metadata":{"id":"GRRpIfcrUkhH","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_env = HexapodV0_3(xml_path=xml_path)\n\njit_reset = jax.jit(eval_env.reset)\njit_step = jax.jit(eval_env.step)","metadata":{"id":"hNqYbcncUqef","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rng = jax.random.PRNGKey(0)\nstate = jit_reset(rng)\nrollout = [state.pipeline_state]\n\n# grab a trajectory\nn_steps = 1000\nrender_every = 1\n\nfor i in range(n_steps):\n  act_rng, rng = jax.random.split(rng)\n  ctrl, _ = jit_inference_fn(state.obs, act_rng)\n  # for i in range(env._physics_steps_per_control_step):\n  state = jit_step(state, ctrl)\n  rollout.append(state.pipeline_state)\n\n  if state.done:\n    break\n\nmedia.show_video(env.render(rollout[::1], camera='hexapod_camera'), fps=1.0 / env.dt / 2)","metadata":{"id":"u-EezEjtVKFY","trusted":true},"execution_count":null,"outputs":[]}]}